{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from matplotlib import rcParams\n",
    "from skimage.measure import regionprops_table\n",
    "\n",
    "rcParams.update({'figure.autolayout': True})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load and transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(\n",
    "    \"data/gt_pred_ie_consolidated.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_2 = merged_df.copy()\n",
    "merged_df_2.rename(columns={\"gt_noisy\": \"silver_standard\", \"gt_proofread\": \"gold_standard\"}, inplace=True)\n",
    "\n",
    "merged_df_2[\"gold_standard\"] = merged_df_2[\"gold_standard\"].round(0)\n",
    "merged_df_2 = merged_df_2[merged_df_2[\"gold_standard\"] < 2]\n",
    "\n",
    "# get rid of Ki67, IDO, PDL1 in decidua because they contain labelling errors\n",
    "merged_df_2 = merged_df_2[~np.logical_and(merged_df_2[\"dataset\"] == \"decidua\", merged_df_2[\"channel\"].isin([\"Ki67\", \"IDO\", \"PDL1\"]))]\n",
    "merged_df_2[\"prediction_binary\"] = (merged_df_2[\"nimbus\"] >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def calc_scores(gt, pred, threshold):\n",
    "    \"\"\"Calculate scores for a given threshold\n",
    "    Args:\n",
    "        gt (np.array):\n",
    "            ground truth labels\n",
    "        pred (np.array):\n",
    "            predictions\n",
    "        threshold (float):\n",
    "            threshold for predictions\n",
    "    Returns:\n",
    "        scores (dict):\n",
    "            dictionary containing scores\n",
    "    \"\"\"\n",
    "    # exclude masked out regions from metric calculation\n",
    "    pred = pred[gt < 2]\n",
    "    gt = gt[gt < 2]\n",
    "    tn, fp, fn, tp = confusion_matrix(\n",
    "        y_true=gt, y_pred=(pred >= threshold).astype(int), labels=[0, 1]\n",
    "    ).ravel()\n",
    "    metrics = {\n",
    "        \"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn,\n",
    "        \"accuracy\": (tp + tn) / (tp + tn + fp + fn + 1e-8),\n",
    "        \"precision\": tp / (tp + fp + 1e-8),\n",
    "        \"recall\": tp / (tp + fn + 1e-8),\n",
    "        \"specificity\": tn / (tn + fp + 1e-8),\n",
    "        \"f1_score\": 2 * tp / (2 * tp + fp + fn + 1e-8),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "results = {}\n",
    "for dataset in merged_df_2[\"dataset\"].unique():\n",
    "    tmp_df = merged_df_2[merged_df_2[\"dataset\"] == dataset]\n",
    "    tmp_df = tmp_df[tmp_df[\"silver_standard\"] != 2]\n",
    "    scores = calc_scores(tmp_df[\"gold_standard\"].values.astype(np.uint8), tmp_df[\"nimbus\"].astype(np.float32), threshold = 0.5)\n",
    "    if scores[\"tp\"] + scores[\"fn\"] > 0:\n",
    "        results[dataset] = scores\n",
    "\n",
    "pred_vs_gold_df = pd.DataFrame(results).T\n",
    "pred_vs_gold_df\n",
    "\n",
    "results = {}\n",
    "for dataset in merged_df_2[\"dataset\"].unique():\n",
    "    tmp_df = merged_df_2[merged_df_2[\"dataset\"] == dataset]\n",
    "    tmp_df = tmp_df[tmp_df[\"silver_standard\"] != 2]\n",
    "    scores = calc_scores(tmp_df[\"gold_standard\"].values.astype(np.uint8), tmp_df[\"silver_standard\"].astype(np.float32), threshold = 0.5)\n",
    "    if scores[\"tp\"] + scores[\"fn\"] > 0:\n",
    "        results[dataset] = scores\n",
    "silver_vs_gold_df = pd.DataFrame(results).T\n",
    "silver_vs_gold_df\n",
    "\n",
    "tmp_df = {}\n",
    "for m in [\"f1_score\", \"precision\", \"recall\", \"specificity\"]:\n",
    "    tmp_df[m] = {\n",
    "        \"Nimbus\": pred_vs_gold_df[m].mean(),\n",
    "        \"Silver standard\": silver_vs_gold_df[m].mean(),\n",
    "    }\n",
    "tmp_df = pd.DataFrame(tmp_df).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def calc_scores(gt, pred, threshold):\n",
    "    \"\"\"Calculate scores for a given threshold\n",
    "    Args:\n",
    "        gt (np.array):\n",
    "            ground truth labels\n",
    "        pred (np.array):\n",
    "            predictions\n",
    "        threshold (float):\n",
    "            threshold for predictions\n",
    "    Returns:\n",
    "        scores (dict):\n",
    "            dictionary containing scores\n",
    "    \"\"\"\n",
    "    # exclude masked out regions from metric calculation\n",
    "    pred = pred[gt < 2]\n",
    "    gt = gt[gt < 2]\n",
    "    tn, fp, fn, tp = confusion_matrix(\n",
    "        y_true=gt, y_pred=(pred >= threshold).astype(int), labels=[0, 1]\n",
    "    ).ravel()\n",
    "    metrics = {\n",
    "        \"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn,\n",
    "        \"accuracy\": (tp + tn) / (tp + tn + fp + fn + 1e-8),\n",
    "        \"precision\": tp / (tp + fp + 1e-8),\n",
    "        \"recall\": tp / (tp + fn + 1e-8),\n",
    "        \"specificity\": tn / (tn + fp + 1e-8),\n",
    "        \"f1_score\": 2 * tp / (2 * tp + fp + fn + 1e-8),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "results = {}\n",
    "for dataset in merged_df_2[\"dataset\"].unique():\n",
    "    tmp_df = merged_df_2[merged_df_2[\"dataset\"] == dataset]\n",
    "    tmp_df = tmp_df[tmp_df[\"silver_standard\"] != 2]\n",
    "    for channel in tmp_df[\"channel\"].unique():\n",
    "        scores = calc_scores(tmp_df[tmp_df[\"channel\"] == channel][\"gold_standard\"].values.astype(np.uint8), tmp_df[tmp_df[\"channel\"] == channel][\"nimbus\"].astype(np.float32), threshold = 0.5)\n",
    "        if scores[\"tp\"] + scores[\"fn\"] > 0:\n",
    "            results[(dataset, channel)] = scores\n",
    "\n",
    "pred_vs_gold_df = pd.DataFrame(results).T\n",
    "\n",
    "results = {}\n",
    "for dataset in merged_df_2[\"dataset\"].unique():\n",
    "    tmp_df = merged_df_2[merged_df_2[\"dataset\"] == dataset]\n",
    "    tmp_df = tmp_df[tmp_df[\"silver_standard\"] != 2]\n",
    "    for channel in tmp_df[\"channel\"].unique():\n",
    "        scores = calc_scores(tmp_df[tmp_df[\"channel\"] == channel][\"gold_standard\"].values.astype(np.uint8), tmp_df[tmp_df[\"channel\"] == channel][\"silver_standard\"].astype(np.float32), threshold = 0.5)\n",
    "        if scores[\"tp\"] + scores[\"fn\"] > 0:\n",
    "            results[(dataset, channel)] = scores\n",
    "silver_vs_gold_df = pd.DataFrame(results).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supplement figures 2 a-e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dataset = {'all': \"Pan-M\",\n",
    " 'codex_colon': \"Codex Colon\",\n",
    " 'vectra_colon': \"Vectra Colon\",\n",
    " 'vectra_pancreas': \"Vectra Pancreas\",\n",
    " 'mibi_breast': \"MIBI-TOF Breast\",\n",
    " 'mibi_decidua': \"MIBI-TOF Decidua\"}\n",
    "\n",
    "pred_vs_gold_df[\"f1_score\"]\n",
    "silver_vs_gold_df[\"f1_score\"]\n",
    "# plot grouped bar chart for f1 score\n",
    "tmp_df = pd.concat([pred_vs_gold_df[\"f1_score\"], silver_vs_gold_df[\"f1_score\"]], axis=1)\n",
    "tmp_df.columns = [\"Nimbus\", \"Silver standard\"]\n",
    "tmp_df.reset_index(inplace=True)\n",
    "tmp_df.rename(columns={\"level_0\": \"dataset\", \"level_1\": \"channel\"}, inplace=True)\n",
    "tmp_df = pd.melt(tmp_df, id_vars=[\"dataset\", \"channel\"], value_vars=[\"Nimbus\", \"Silver standard\"], var_name=\"Model\")\n",
    "tmp_df.rename(columns={\"value\": \"F1 score\"}, inplace=True)\n",
    "tmp_df.channel.replace({\"panCK+CK7+CAM5.2\": \"panCK\"}, inplace=True)\n",
    "ratios = tmp_df.groupby(\"dataset\")[\"channel\"].nunique().values\n",
    "os.makedirs(\"figures/supplement\", exist_ok=True)\n",
    "for dataset in tmp_df.dataset.unique():\n",
    "    tmp_df_2 = tmp_df[tmp_df[\"dataset\"] == dataset]\n",
    "    fig, ax = plt.subplots()\n",
    "    tmp_df_2.sort_values(\"F1 score\", inplace=True, ascending=False)\n",
    "    p = sns.barplot(data=tmp_df_2, x=\"channel\", y=\"F1 score\", hue=\"Model\", hue_order=[\"Nimbus\", \"Silver standard\"], width=tmp_df_2.channel.nunique()*0.02, ax=ax)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(rename_dataset[dataset], fontsize=18)\n",
    "    plt.xlabel(\"Channel\", fontsize=16)\n",
    "    plt.ylabel(\"F1 score\", fontsize=16)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(loc='lower left', fontsize=14)\n",
    "    plt.savefig(f\"figures/supplement/f1_score_split_by_channel_{dataset}.svg\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 4 d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "# result storage dicts\n",
    "results_gs = {}\n",
    "results_ss = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make visualization of cell wise f1 score\n",
    "merged_df_3 = merged_df_2[np.logical_and(merged_df_2[\"gold_standard\"] != 2, merged_df_2[\"silver_standard\"] != 2)]\n",
    "merged_df_3[\"error_gold\"] = np.abs(merged_df_3[\"prediction_binary\"] - merged_df_3[\"gold_standard\"]).astype(int)\n",
    "merged_df_3[\"error_silver\"] = np.abs(merged_df_3[\"prediction_binary\"] - merged_df_3[\"silver_standard\"]).astype(int)\n",
    "\n",
    "results_gs[\"cell_size\"] = mutual_info_classif(merged_df_3[\"cell_size\"].values.reshape(-1, 1), merged_df_3[\"error_gold\"])\n",
    "results_ss[\"cell_size\"] = mutual_info_classif(merged_df_3[\"cell_size\"].values.reshape(-1, 1), merged_df_3[\"error_silver\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Marker heterogeneity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  marker heterogeneity measured by the coefficient of variation\n",
    "heterogeneity = merged_df_3[merged_df_3[\"gold_standard\"] == 1.0].groupby([\"dataset\", \"channel\", \"fov\"], as_index=True).apply(lambda x: x.ie.std()/(x.ie.mean()+1e-8)).to_dict()\n",
    "merged_df_3[\"heterogeneity\"] = merged_df_3.apply(lambda x: np.abs(heterogeneity[(x[\"dataset\"], x[\"channel\"], x[\"fov\"])]) if (x[\"dataset\"], x[\"channel\"], x[\"fov\"]) in heterogeneity.keys() else None, axis=1)\n",
    "\n",
    "isna = merged_df_3[\"heterogeneity\"].isna()\n",
    "results_gs[\"heterogeneity\"] = mutual_info_classif(merged_df_3[~isna][\"heterogeneity\"].values.reshape(-1, 1), merged_df_3[~isna][\"error_gold\"])\n",
    "results_ss[\"heterogeneity\"] = mutual_info_classif(merged_df_3[~isna][\"heterogeneity\"].values.reshape(-1, 1), merged_df_3[~isna][\"error_silver\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rare markers: Low share of cells that are GT positive for the marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rarity = merged_df_3.groupby([\"dataset\", \"channel\", \"fov\"], as_index=True).mean(\"gold_standard\")[\"gold_standard\"]\n",
    "rarity_dict = rarity.to_dict()\n",
    "merged_df_3[\"rarity\"] = merged_df_3.apply(lambda x: rarity_dict[(x[\"dataset\"], x[\"channel\"], x[\"fov\"])], axis=1)\n",
    "\n",
    "results_gs[\"rarity\"] = mutual_info_classif(merged_df_3[\"rarity\"].values.reshape(-1, 1), merged_df_3[\"error_gold\"])\n",
    "results_ss[\"rarity\"] = mutual_info_classif(merged_df_3[\"rarity\"].values.reshape(-1, 1), merged_df_3[\"error_silver\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Marker sparsity\n",
    "Share of marker positive cells in the region of a cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageio import imread\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def mskcc_pancreas_naming_convention(fname):\n",
    "    return os.path.join(\n",
    "        \"data/MSKCC_pancreas/segmentation\",\n",
    "        fname + \"feature_0.ome.tif\"\n",
    "    )\n",
    "\n",
    "def mskcc_colon_naming_convention(fname):\n",
    "    return os.path.join(\n",
    "        \"data/MSKCC_colon/segmentation\",\n",
    "        fname + \"feature_0.ome.tif\"\n",
    "    )\n",
    "\n",
    "def hickey_naming_convention(fov_path):\n",
    "    fname = os.path.basename(fov_path)\n",
    "    fov, reg = fname.split(\"_\")[:2]\n",
    "    fov_path = os.path.join(\"data/hickey/masks\", fov)\n",
    "    images = os.listdir(fov_path)\n",
    "    labels = [img for img in images if \"_labeled\" in img]\n",
    "    labels = [img for img in labels if reg in img]\n",
    "    label_fname = labels[0]    \n",
    "    return os.path.join(os.path.normpath(fov_path), label_fname)\n",
    "\n",
    "def decidua_naming_convention(fov_path):\n",
    "    \"\"\"Prepares the path to the segmentation data for a given fov\n",
    "    Args:\n",
    "        fov_path (str): path to fov\n",
    "    Returns:\n",
    "        seg_path (str): paths to segmentation fovs\n",
    "    \"\"\"\n",
    "    base_dir = \"data/decidua\"\n",
    "    deepcell_output_dir = os.path.join(base_dir, \"segmentation_data\")\n",
    "    fov_name = os.path.basename(fov_path)\n",
    "    return os.path.join(\n",
    "        deepcell_output_dir, fov_name + \"_segmentation_labels.tiff\"\n",
    "    )\n",
    "\n",
    "def tonic_naming_convention(fov_name):\n",
    "    return os.path.join(\n",
    "        os.path.normpath(\n",
    "            \"data/TONIC/segmentation_data/deepcell_output\"\n",
    "        ), fov_name + \"_feature_0.tif\"\n",
    "    )\n",
    "\n",
    "naming_convention = {\n",
    "    'hickey': hickey_naming_convention,\n",
    "    'mskcc_colon': mskcc_colon_naming_convention,\n",
    "    'mskcc_pancreas': mskcc_pancreas_naming_convention,\n",
    "    'tonic': tonic_naming_convention,\n",
    "    'decidua': decidua_naming_convention\n",
    "}\n",
    "fovs = {\n",
    "    'hickey': [\n",
    "        'B010A_reg003_X01_Y01_Z01',\n",
    "        'B011B_reg001_X01_Y01_Z01',\n",
    "        'B011B_reg003_X01_Y01_Z01'\n",
    "    ],\n",
    "    'mskcc_colon': [\n",
    "        '3e507f0a3dd2_Colon P20 CD3, Foxp1, PDL1, ICOS, CD8, panCK+CK7+CAM5.2__[54006,21157]_image',\n",
    "        '49e532ac63a8_3-13 Colon P20 CD3, Foxp1, PDL1, ICOS, CD8, panCK+CK7+CAM5.2__[53850,11905]_image',\n",
    "        '9c68495d8667_Colon P20 CD3, Foxp1, PDL1, ICOS, CD8, panCK+CK7+CAM5.2__[55647,17034]_image'\n",
    "    ],\n",
    "    'mskcc_pancreas': [\n",
    "        '0852a4103bed Pancreas_PANEL7-10_CD40L,_CD40,_PD1,_PDL1,CD8,CK_[61352,11423]_component_data.tif_image',\n",
    "        '8f39cecaa5aa Pancreas_PANEL7-10_CD40L,_CD40,_PD1,_PDL1,CD8,CK_[43899,11766]_component_data.tif_image',\n",
    "        'ce418553b719 Pancreas_PANEL7-10_CD40L,_CD40,_PD1,_PDL1,CD8,CK_[55606,14580]_component_data.tif_image',\n",
    "    ],\n",
    "    'tonic': [\n",
    "        'TONIC_TMA10_R1C1',\n",
    "        'TONIC_TMA10_R3C6',\n",
    "        'TONIC_TMA10_R5C4'\n",
    "    ],\n",
    "    'decidua': [\n",
    "        '12_31750_16_12',\n",
    "        '12_31750_1_10',\n",
    "        '14_31758_20_4'\n",
    "    ]\n",
    "\n",
    "}\n",
    "\n",
    "fov_list = []\n",
    "\n",
    "for dataset in fovs.keys():\n",
    "    for fov in tqdm(fovs[dataset]):\n",
    "        inst_seg_path = naming_convention[dataset](fov)\n",
    "        inst_seg = imread(inst_seg_path)\n",
    "        inst_seg = np.squeeze(inst_seg.astype(np.uint16))\n",
    "        df = pd.DataFrame(regionprops_table(inst_seg, properties=[\"label\", \"centroid\"]))\n",
    "        df[\"fov\"] = fov\n",
    "        df[\"dataset\"] = dataset\n",
    "        fov_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate for each cell the number of cells in a 120 pixel radius\n",
    "from copy import deepcopy\n",
    "\n",
    "regionprops_df = pd.concat(fov_list)\n",
    "regionprops_df.rename(columns={\"label\": \"labels\"}, inplace=True)\n",
    "merged_df_4 = merged_df_3.merge(regionprops_df, on=[\"dataset\", \"fov\", \"labels\"], how=\"left\")\n",
    "\n",
    "for radius in [120]:\n",
    "    merged_df_4[\"num_cells_region\"] = np.nan\n",
    "    for dataset in merged_df_4[\"dataset\"].unique():\n",
    "        for fov in merged_df_4[\"fov\"].unique():\n",
    "            df_tmp = merged_df_4[np.logical_and(merged_df_4[\"dataset\"] == dataset, merged_df_4[\"fov\"] == fov)]\n",
    "            for marker in tqdm(df_tmp.channel.unique()):\n",
    "                df_tmp_ = df_tmp[df_tmp[\"channel\"] == marker]\n",
    "                def calc_num_cells_in_radius(row, radius=radius):\n",
    "                    h_tmp = df_tmp_[df_tmp_[\"centroid-0\"].between(row[\"centroid-0\"]-radius, row[\"centroid-0\"]+radius, inclusive=\"both\")]\n",
    "                    w_tmp = h_tmp[h_tmp[\"centroid-1\"].between(row[\"centroid-1\"]-radius, row[\"centroid-1\"]+radius, inclusive=\"both\")]\n",
    "                    return np.sum(w_tmp[\"gold_standard\"] == 1.0)\n",
    "                num_cells_region = df_tmp_.apply(lambda x: calc_num_cells_in_radius(x, radius), axis=1)\n",
    "                merged_df_4.loc[num_cells_region.index, \"num_pos_cells_region\"] = num_cells_region.values.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gs[\"sparsity\"] = mutual_info_classif(merged_df_4[\"num_pos_cells_region\"].values.reshape(-1, 1), merged_df_4[\"error_gold\"])\n",
    "results_ss[\"sparsity\"] = mutual_info_classif(merged_df_4[\"num_pos_cells_region\"].values.reshape(-1, 1), merged_df_4[\"error_silver\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate for each cell the number of cells in a 40 pixel radius\n",
    "from copy import deepcopy\n",
    "\n",
    "regionprops_df = pd.concat(fov_list)\n",
    "regionprops_df.rename(columns={\"label\": \"labels\"}, inplace=True)\n",
    "\n",
    "\n",
    "density_results = {}\n",
    "for radius in [120]:\n",
    "    merged_df_4[\"num_cells_region\"] = np.nan\n",
    "    for dataset in merged_df_4[\"dataset\"].unique():\n",
    "        for fov in merged_df_4[\"fov\"].unique():\n",
    "            df_tmp = merged_df_4[np.logical_and(merged_df_4[\"dataset\"] == dataset, merged_df_4[\"fov\"] == fov)]\n",
    "            for marker in tqdm(df_tmp.channel.unique()):\n",
    "                df_tmp_ = df_tmp[df_tmp[\"channel\"] == marker]\n",
    "                def calc_num_cells_in_radius(row, radius=radius):\n",
    "                    h_tmp = df_tmp_[df_tmp_[\"centroid-0\"].between(row[\"centroid-0\"]-radius, row[\"centroid-0\"]+radius, inclusive=\"both\")]\n",
    "                    w_tmp = h_tmp[h_tmp[\"centroid-1\"].between(row[\"centroid-1\"]-radius, row[\"centroid-1\"]+radius, inclusive=\"both\")]\n",
    "                    return np.sum(w_tmp[\"gold_standard\"] != 5.0)\n",
    "                num_cells_region = df_tmp_.apply(lambda x: calc_num_cells_in_radius(x, radius), axis=1)\n",
    "                merged_df_4.loc[num_cells_region.index, \"num_cells_region\"] = num_cells_region.values.astype(np.int32)\n",
    "    density_results[radius] = deepcopy(merged_df_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gs[\"density\"] = mutual_info_classif(merged_df_4[\"num_cells_region\"].values.reshape(-1, 1), merged_df_4[\"error_gold\"])\n",
    "results_ss[\"density\"] = mutual_info_classif(merged_df_4[\"num_cells_region\"].values.reshape(-1, 1), merged_df_4[\"error_silver\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot final results for Figure 3 d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_gs = pd.DataFrame(results_gs).T\n",
    "result_df_ss = pd.DataFrame(results_ss).T\n",
    "result_df_gs.rename(columns={0: \"Mutual information\"}, inplace=True)\n",
    "result_df_ss.rename(columns={0: \"Mutual information\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gs = result_df_gs\n",
    "df_ss = result_df_ss\n",
    "\n",
    "df = pd.DataFrame({\"Nimbus\":df_gs[\"Mutual information\"].values, \"Silver standard\": df_ss[\"Mutual information\"].values}, index=df_ss.index)\n",
    "df = df.loc[[\"cell_size\", \"heterogeneity\", \"rarity\", \"sparsity\", \"density\"], :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gs = result_df_gs\n",
    "df_ss = result_df_ss\n",
    "\n",
    "df = pd.DataFrame({\"Nimbus\":df_gs[\"Mutual information\"].values, \"Silver standard\": df_ss[\"Mutual information\"].values}, index=df_ss.index)\n",
    "df = df.loc[[\"cell_size\", \"heterogeneity\", \"rarity\", \"sparsity\", \"density\"], :]\n",
    "\n",
    "font = {'family' : 'sans-serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 12}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (5.5, 5.5))\n",
    "im = ax.imshow(df.values, cmap=\"Reds\", vmin=0., vmax=1.)\n",
    "ax.set_xticks(np.arange(len(df.columns)), labels=[\"Nimbus\", \"Integrated\\n Expression\"], rotation=45)\n",
    "ax.xaxis.tick_bottom()\n",
    "ax.set_yticks(np.arange(len(df.index)), labels=[\"Smaller cells\", \"Marker \\n heterogeneity\", \"Marker rarity\", \"Marker Sparsity\", \"Cell Density\"])\n",
    "# Rotate the tick labels and set their alignment.\n",
    "\n",
    "for ii, i in enumerate(df.index):\n",
    "    for jj, j in enumerate(df.columns):\n",
    "        text = ax.text(jj, ii, df.loc[i][j].round(4),\n",
    "                       ha=\"center\", va=\"center\", color=\"black\")\n",
    "cb1 = plt.colorbar(im, orientation=\"vertical\", ticks=[1.0, 0.0])\n",
    "cb1.ax.invert_xaxis()\n",
    "ax.yaxis.set_label_position(\"right\")\n",
    "ax.set_ylabel(\"Mutual information\", rotation=270, labelpad=16)\n",
    "ax.set_title(\"Impact of confounders on\\n classification errors\")\n",
    "plt.savefig(\"figures/figure_3/error_x_confounders_mutual_information.svg\", dpi=300, bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Nimbus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
