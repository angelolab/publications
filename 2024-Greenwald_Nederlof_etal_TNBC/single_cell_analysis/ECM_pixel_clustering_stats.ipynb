{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ce9e5da-7302-47eb-84b5-f5ae5c1d82b0",
   "metadata": {},
   "source": [
    "# ECM pixel cluster stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a86907-b8fe-44e5-93d4-2f5389ddecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/Volumes/Shared/Noah Greenwald/TONIC_Cohort/intermediate_files/ecm_pixel_clustering\"\n",
    "tiff_dir = \"/Volumes/Shared/Noah Greenwald/TONIC_Cohort/image_data/samples\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e07435-1967-416c-832b-0b79b3e387d1",
   "metadata": {},
   "source": [
    "## Pixel cluster counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a195bdef-89d5-4660-956b-33ce13ea99e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Count pixel clusters\n",
    "\n",
    "Author: Candace Liu\n",
    "Date: 6/19/23\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pixie_dir = os.path.join(base_dir, \"pixie\", \"ecm_061423_pixel_output_dir\")\n",
    "all_fovs = os.listdir(os.path.join(pixie_dir, \"pixel_masks\"))\n",
    "all_fovs = [x.replace(\"_pixel_mask.tiff\",\"\") for x in all_fovs]\n",
    "\n",
    "all_df = []\n",
    "for fov in all_fovs:\n",
    "    print(fov)\n",
    "    one_fov = io.imread(os.path.join(pixie_dir, \"pixel_masks\", fov+\"_pixel_mask.tiff\"))\n",
    "    cluster, counts = np.unique(one_fov, return_counts=True)\n",
    "    one_dat = {'pixel_meta_cluster':cluster, 'counts':counts}\n",
    "    df = pd.DataFrame(one_dat)\n",
    "    df['fov'] = fov\n",
    "    all_df.append(df)\n",
    "\n",
    "all_dat = pd.concat(all_df, ignore_index=True)\n",
    "\n",
    "# Add pixel meta cluster names\n",
    "mapping = pd.read_csv(os.path.join(pixie_dir, \"pixel_channel_avg_meta_cluster.csv\"))\n",
    "mapping_unique = mapping[['pixel_meta_cluster','pixel_meta_cluster_rename']].drop_duplicates()\n",
    "all_dat = pd.merge(all_dat,mapping_unique, on='pixel_meta_cluster')\n",
    "all_dat.to_csv(os.path.join(pixie_dir, \"fov_pixel_cluster_counts.csv\"), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d912a1-0f8f-4c55-8ef8-e46b47aeb84c",
   "metadata": {},
   "source": [
    "## Shape Analysis\n",
    "### Save binary cluster tiffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5831847-1d07-4359-889b-618db06d6264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save 1 binary TIFF per cluster\n",
    "\n",
    "Author: Candace Liu\n",
    "Date: 6/14/23\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from PIL import Image\n",
    "import skimage.io as io\n",
    "\n",
    "pixie_dir = os.path.join(base_dir, \"pixie\", \"ecm_061423_pixel_output_dir\")\n",
    "pixel_mask_dir = os.path.join(pixie_dir, \"pixel_masks\")\n",
    "clust_to_pheno_path = os.path.join(pixie_dir, \"pixel_channel_avg_meta_cluster.csv\")\n",
    "\n",
    "shape_analysis_dir = os.path.join(base_dir, \"shape_analysis\")\n",
    "output_dir = os.path.join(shape_analysis_dir, \"one_cluster_masks\")\n",
    "\n",
    "# Get phenotype mapping\n",
    "clust_to_pheno = pd.read_csv(clust_to_pheno_path)\n",
    "clusters_to_name = pd.Series(clust_to_pheno.pixel_meta_cluster_rename.values, index=clust_to_pheno.pixel_meta_cluster).to_dict()\n",
    "\n",
    "# Make output directory\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "all_fovs = os.listdir(pixel_mask_dir)\n",
    "all_fovs = [x for x in all_fovs if \"_pixel_mask.tiff\" in x]\n",
    "all_fovs = [x.replace(\"_pixel_mask.tiff\",\"\") for x in all_fovs]\n",
    "for fov in all_fovs:\n",
    "\n",
    "    if not os.path.exists(os.path.join(output_dir,fov)):\n",
    "        os.makedirs(os.path.join(output_dir,fov))\n",
    "\n",
    "    # Read in pixel mask\n",
    "    clust_array = np.array(io.imread(os.path.join(pixel_mask_dir,fov+\"_pixel_mask.tiff\")))\n",
    "    fov_clusters = np.unique(clust_array)\n",
    "    fov_clusters = [x for x in fov_clusters if x!=0]\n",
    "\n",
    "    for clust in fov_clusters:\n",
    "        clust_name = clusters_to_name[clust]\n",
    "        one_clust_array = np.copy(clust_array)\n",
    "        one_clust_array[np.where(one_clust_array != clust)] = 0\n",
    "        one_clust_array[np.where(one_clust_array == clust)] = 1\n",
    "       \n",
    "        # Save overlay as TIF\n",
    "        im = Image.fromarray(one_clust_array.astype(np.int16))\n",
    "        im.save(os.path.join(output_dir, fov, clust_name+\".tiff\"))\n",
    "\n",
    "    print(fov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa25b4-97fc-49a8-9137-48218886d528",
   "metadata": {},
   "source": [
    "### Extract properties from connected objects in cluster masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7921fc82-2877-4401-bd12-5ecb392e02e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from skimage.measure import label, regionprops_table\n",
    "import skimage.io as io\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "shape_analysis_dir = os.path.join(base_dir, \"shape_analysis\")\n",
    "\n",
    "one_cluster_dir = os.path.join(shape_analysis_dir, \"one_cluster_masks\")\n",
    "output_dir = os.path.join(shape_analysis_dir, \"object_masks\")\n",
    "\n",
    "all_fovs = os.listdir(one_cluster_dir)\n",
    "all_clusters = os.listdir(os.path.join(one_cluster_dir,all_fovs[0]))\n",
    "all_clusters = [x.replace(\".tiff\",\"\") for x in all_clusters]\n",
    "\n",
    "for fov in all_fovs:\n",
    "    print(fov)\n",
    "\n",
    "    if not os.path.exists(os.path.join(output_dir,fov)):\n",
    "        os.makedirs(os.path.join(output_dir,fov))\n",
    "\n",
    "    for cluster in all_clusters:\n",
    "        if os.path.exists(os.path.join(one_cluster_dir,fov,cluster+\".tiff\")):\n",
    "            binary_im = io.imread(os.path.join(one_cluster_dir,fov,cluster+\".tiff\"))\n",
    "            labels = label(binary_im, connectivity=2)\n",
    "\n",
    "            # Save object mask\n",
    "            im = Image.fromarray(labels.astype(np.int16))\n",
    "            im.save(os.path.join(output_dir,fov,cluster+\".tiff\"))\n",
    "\n",
    "            # Extract region props\n",
    "            tab = regionprops_table(labels, properties=('label',\n",
    "                                                        'area',\n",
    "                                                        'centroid',\n",
    "                                                        'perimeter',\n",
    "                                                        'major_axis_length',\n",
    "                                                        'minor_axis_length'))\n",
    "            tab = pd.DataFrame(tab)\n",
    "            tab.to_csv(os.path.join(output_dir,fov,cluster+\"_regions.csv\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f60f78-f33a-432f-b750-e4eb092a2db2",
   "metadata": {},
   "source": [
    "### Stats calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d29419-cedc-4f6e-a237-07b433194648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Quantify pixel cluster object masks\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from alpineer import io_utils\n",
    "\n",
    "shape_analysis_dir = os.path.join(base_dir, \"shape_analysis\")\n",
    "\n",
    "shape_mask_dir = os.path.join(shape_analysis_dir, 'object_masks')\n",
    "fovs = io_utils.list_folders(shape_mask_dir)\n",
    "\n",
    "clusters = ['Collagen', 'FAP', 'FAP_Collagen', 'FAP_Fibronectin', 'FAP_SMA', 'Fibronectin', 'Fibronectin_Collagen', \n",
    "            'SMA', 'SMA_Collagen', 'SMA_Fibronectin', 'Vim', 'Vim_Collagen', 'Vim_FAP', 'Vim_Fibronectin', 'Vim_SMA']\n",
    "\n",
    "\n",
    "all_data = []\n",
    "for fov in fovs:\n",
    "    fov_dir = os.path.join(shape_mask_dir, fov)\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        cluster_object_table = pd.read_csv(os.path.join(fov_dir, cluster + '_regions.csv'))\n",
    "        cluster_object_table_new = cluster_object_table[['fov', 'area', 'major_axis_length', 'minor_axis_length']]\n",
    "        \n",
    "        cluster_object_table_new = cluster_object_table_new.rename({'major_axis_length': 'major_axis', \n",
    "                                                                    'minor_axis_length': 'minor_axis'})\n",
    "        \n",
    "        cluster_object_table_new['cluster'] = cluster\n",
    "        cluster_object_table_new['axis_ratio'] = cluster_object_table_new.major_axis / cluster_object_table_new.minor_axis\n",
    "        cluster_object_table_new['axis_diff_norm'] = (cluster_object_table_new.major_axis - cluster_object_table_new.minor_axis)/ cluster_object_table_new.area\n",
    "        \n",
    "        all_data.append(cluster_object_table_new)\n",
    "        \n",
    "all_data = pd.concat(all_data)\n",
    "all_data.to_csv(os.path.join(shape_mask_dir, 'object_mask_properties.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e821fae8-6785-4ad1-a0d6-4a20ed861b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summarize FOV level data\n",
    "\n",
    "mask_data = pd.read_csv(os.path.join(shape_mask_dir, \"object_mask_properties.csv\"))\n",
    "\n",
    "# Filter for objects with area greater than 10\n",
    "mask_data = mask_data[mask_data.area > 10]\n",
    "\n",
    "cluster_stats = mask_data[['fov', 'cluster', 'axis_ratio', 'axis_diff_norm']].groupby(by=['fov', 'cluster']).mean().reset_index()\n",
    "cluster_stats = cluster_stats.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "ratio_mean = cluster_stats[['fov', 'cluster', 'axis_ratio']].dropna()\n",
    "ratio_mean.to_csv(os.path.join(shape_mask_dir,  \"fov_object_mean_ratio.csv\"), index=False)\n",
    "\n",
    "diff_norm_mean = cluster_stats[['fov', 'cluster', 'axis_diff_norm']].dropna()\n",
    "diff_norm_mean.to_csv(os.path.join(shape_mask_dir,  \"fov_object_mean_diff_norm.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaa5468-58e9-4494-a75d-932df83a284b",
   "metadata": {},
   "source": [
    "## Neighborhood Analysis\n",
    "### Kmeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb1c45-7a10-43d5-b202-0ee82483b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import feather\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "pixie_dir = os.path.join(base_dir, \"pixie\", \"ecm_061423_pixel_output_dir\")\n",
    "cluster_overlay_dir = os.path.join(pixie_dir, \"pixel_masks\")\n",
    "step_size = 1\n",
    "window_size = 50\n",
    "output_dir = \"neighborhood_mats\"+\"_window\"+str(window_size)\n",
    "subset_dir = \"neighborhood_mats_subset\"+\"_window\"+str(window_size)\n",
    "training_subset = 0.1\n",
    "seed = 329\n",
    "\n",
    "# Make output directory\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "if not os.path.exists(subset_dir):\n",
    "    os.makedirs(subset_dir)\n",
    "\n",
    "# Get all fovs\n",
    "all_fovs = os.listdir(cluster_overlay_dir)\n",
    "all_fovs = [x for x in all_fovs if \"_pixel_mask.tiff\" in x]\n",
    "all_fovs = [x.replace(\"_pixel_mask.tiff\",\"\") for x in all_fovs]\n",
    "# Get completed fovs\n",
    "done_fovs = os.listdir(subset_dir)\n",
    "done_fovs = [x for x in done_fovs if \".feather\" in x]\n",
    "done_fovs = [x.replace(\".feather\",\"\") for x in done_fovs]\n",
    "# Only keep fovs that have not been completed\n",
    "fovs = [x for x in all_fovs if x not in done_fovs]\n",
    "\n",
    "# Get cluster numbers\n",
    "cluster_data = pd.read_csv(os.path.join(pixie_dir, \"pixel_channel_avg_meta_cluster.csv\"))\n",
    "clusters = cluster_data['pixel_meta_cluster'].values\n",
    "\n",
    "# Create generator for all windows\n",
    "def sliding_window(image, windowSize):\n",
    "    for x in range(0, image.shape[0]):\n",
    "        startx = x-windowSize\n",
    "        if startx < 0:\n",
    "            startx = 0\n",
    "        endx = x+windowSize\n",
    "        if endx > image.shape[1]:\n",
    "            endx = image.shape[1]\n",
    "        for y in range(0, image.shape[1]):\n",
    "            # Don't return if pixel is 0\n",
    "            if image[x,y] != 0:\n",
    "                starty = y-windowSize\n",
    "                if starty < 0:\n",
    "                    starty = 0\n",
    "                endy = y+windowSize\n",
    "                if endy > image.shape[0]:\n",
    "                    endy = image.shape[0]\n",
    "                yield (x, y, startx, endx, starty, endy)\n",
    "\n",
    "# Count clusters in each window\n",
    "def extractFeatures(im, window):\n",
    "    x,y,startx,endx,starty,endy = window\n",
    "    one_im = im[startx:endx, starty:endy]\n",
    "    counts = [np.count_nonzero(one_im == c) for c in clusters]\n",
    "    counts.insert(0,y)\n",
    "    counts.insert(0,x)\n",
    "    return counts\n",
    "\n",
    "# Get neighbors for each pixel\n",
    "def one_fov(fov):\n",
    "    features = []\n",
    "    im = np.array(Image.open(os.path.join(cluster_overlay_dir, fov+\"_pixel_mask.tiff\")))\n",
    "    windows = sliding_window(im, window_size)\n",
    "    for i,window in enumerate(windows):\n",
    "        featureVector = extractFeatures(im,window)\n",
    "        # Sum up features (first two elements are x and y)\n",
    "        if np.sum(featureVector[2:]) != 0:\n",
    "            featureVector.insert(0, fov)\n",
    "            features.append(featureVector)\n",
    "\n",
    "    # Write to file\n",
    "    features_np = np.asarray(features)\n",
    "    df = pd.DataFrame(features_np)\n",
    "    header = ['clust_'+str(c) for c in clusters]\n",
    "    header = ['fov','x','y']+header\n",
    "    df.columns = header\n",
    "    feather.write_dataframe(df, os.path.join(output_dir,fov+'.feather'), compression='uncompressed')\n",
    "\n",
    "    # Get random subset for training\n",
    "    df_subset = df.sample(frac=training_subset, random_state=seed)\n",
    "    feather.write_dataframe(df_subset, os.path.join(subset_dir,fov+'.feather'), compression='uncompressed')\n",
    "\n",
    "results = Parallel(n_jobs=4)(delayed(one_fov)(i) for i in fovs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c91c43f-dcf2-4987-aac3-7f6d158c2ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feather\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from ark.analysis import visualize\n",
    "from scipy.stats import zscore\n",
    "from PIL import Image\n",
    "import joblib\n",
    "\n",
    "train_dat_dir = \"neighborhood_mats_subset_window50\"\n",
    "all_fov_dir = \"neighborhood_mats_window50\"\n",
    "pixie_dir = os.path.join(base_dir, \"pixie\", \"ecm_061423_pixel_output_dir\")\n",
    "\n",
    "clust_to_pheno_path = os.path.join(pixie_dir, \"pixel_channel_avg_meta_cluster.csv\")\n",
    "neighborhood_dir = os.path.join(base_dir, \"neighborhood\", \"neighborhood_masks\")\n",
    "cluster_num  = 5\n",
    "seed = 329\n",
    "\n",
    "# Make output directory\n",
    "if not os.path.exists(neighborhood_dir):\n",
    "    os.makedirs(neighborhood_dir)\n",
    "\n",
    "# Read in data\n",
    "train_fovs = os.listdir(train_dat_dir)\n",
    "train_data = pd.concat(\n",
    "    [feather.read_dataframe(os.path.join(train_dat_dir,fov)) for fov in train_fovs]\n",
    ")\n",
    "train_cols = train_data.columns.values\n",
    "train_cols = [x for x in train_cols if \"clust_\" in x]\n",
    "train_data[train_cols] = train_data[train_cols].astype(np.int32)\n",
    "\n",
    "# Get phenotype mapping\n",
    "clust_to_pheno = pd.read_csv(clust_to_pheno_path)\n",
    "clust_to_pheno_dict = pd.Series(clust_to_pheno.pixel_meta_cluster_rename.values, index=clust_to_pheno.pixel_meta_cluster).to_dict()\n",
    "\n",
    "# k-means clustering\n",
    "cluster_fit = KMeans(n_clusters=cluster_num, random_state=seed, n_init=10).fit(train_data[train_cols])\n",
    "joblib.dump(cluster_fit, \"kmeans_model.joblib\")\n",
    "train_data['neighborhood'] = cluster_fit.labels_ + 1\n",
    "# Make heatmap\n",
    "keep_cols = train_cols+['neighborhood']\n",
    "train_data_keep = train_data[keep_cols]\n",
    "mean_dat = train_data_keep.groupby('neighborhood', as_index=False).mean(numeric_only=True)\n",
    "mean_dat.to_csv(\"neighborhood_cluster_mean.csv\", index=False)\n",
    "mean_dat_values = mean_dat.drop('neighborhood', axis=1)\n",
    "visualize.draw_heatmap(data=mean_dat_values.apply(zscore).values,\n",
    "                       x_labels=[\"Cluster\"+str(x) for x in mean_dat['neighborhood'].values],\n",
    "                       y_labels=[clust_to_pheno_dict[int(x.replace(\"clust_\",\"\"))] for x in mean_dat.drop('neighborhood', axis=1).columns.values],\n",
    "                       center_val=0,\n",
    "                       save_dir=\".\",\n",
    "                       save_file=\"training_k\"+str(cluster_num)+\"_heatmap\")\n",
    "\n",
    "## Assign neighborhood to all FOVs\n",
    "all_fovs = os.listdir(all_fov_dir)\n",
    "all_fovs = [x for x in all_fovs if \".feather\" in x]\n",
    "all_fovs = [x.replace(\".feather\",\"\") for x in all_fovs]\n",
    "\n",
    "# Define function to assign labels\n",
    "def assign_label(clust_array,x,y,clust):\n",
    "    clust_array[x,y] = clust\n",
    "\n",
    "keep_cols = ['x','y']\n",
    "for fov in all_fovs:\n",
    "    print(fov)\n",
    "    one_fov = feather.read_dataframe(os.path.join(all_fov_dir,fov+\".feather\"))\n",
    "    one_fov['neighborhood'] = cluster_fit.predict(one_fov[train_cols])\n",
    "    one_fov[keep_cols] = one_fov[keep_cols].astype(np.int32)\n",
    "    \n",
    "    # Get size\n",
    "    sample_im = np.array(Image.open(os.path.join(tiff_dir,fov,\"Au.tiff\")))\n",
    "    # Create array for cluster labels, fill with 0's\n",
    "    nh_array = np.full((sample_im.shape), 0, dtype=int)\n",
    "    # Fill in array\n",
    "    [assign_label(nh_array,row[0],row[1],row[2]) for row in one_fov[['x','y','neighborhood']].values]\n",
    "    # Save\n",
    "    im = Image.fromarray(nh_array.astype(np.int32))\n",
    "    im.save(os.path.join(neighborhood_dir, fov+\"_nh_mask.tiff\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b47a938-eab6-4c8c-84c7-17cb21f4f2ee",
   "metadata": {},
   "source": [
    "### Generate FOV neighborhood counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be86bf-06d5-41d7-95c6-64f5f14918fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "neighborhood_mask_dir = \"neighborhood_masks\"\n",
    "num_clusters = 5\n",
    "\n",
    "all_fovs = os.listdir(neighborhood_mask_dir)\n",
    "all_fovs = [x for x in all_fovs if \"_nh_mask.tiff\" in x]\n",
    "all_fovs = [x.replace(\"_nh_mask.tiff\",\"\") for x in all_fovs]\n",
    "\n",
    "cluster_cols = ['Cluster'+str(x) for x in range(1,num_clusters+1)]\n",
    "cols = ['fov']+cluster_cols+['total'] \n",
    "all_df = pd.DataFrame(columns=cols)\n",
    "all_df['fov'] = all_fovs\n",
    "\n",
    "for fov in all_fovs:\n",
    "    print(fov)\n",
    "    nh_mask = io.imread(os.path.join(neighborhood_mask_dir, fov+\"_nh_mask.tiff\"))\n",
    "    unique, counts = np.unique(nh_mask, return_counts=True)\n",
    "    count_dict = dict(zip(unique,counts))\n",
    "    search_counts = [count_dict[x] if x in unique else 0 for x in range(1,num_clusters+1)]\n",
    "    search_counts = np.array(search_counts)\n",
    "    fov_df = pd.DataFrame(search_counts.reshape(-1,len(search_counts)), columns=['Cluster'+str(x) for x in range(1,num_clusters+1)])\n",
    "    all_df.loc[all_df[\"fov\"]==fov,cluster_cols] = search_counts\n",
    "    all_df.loc[all_df[\"fov\"]==fov,\"total\"] = np.count_nonzero(nh_mask)\n",
    "\n",
    "all_df.to_csv(os.path.join(neighborhood_dir, \"fov_neighborhood_counts.csv\"), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ark_env",
   "language": "python",
   "name": "ark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
